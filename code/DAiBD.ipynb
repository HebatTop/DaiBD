{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the environment variable for PySpark to the current path of the Python interpreter (sys.executable)\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries and modules\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, IndexedRowMatrix\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.mllib.linalg import DenseMatrix\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.linalg import lu, lu_factor, lu_solve # is used for LU decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and configuration of the Spark session.\n",
    "\n",
    "# Set the master node for Spark (running locally with 4 cores on a Spark standalone cluster).\n",
    "# Using .appName() to set the name for the application, which will be displayed on the user interface.\n",
    "# With .getOrCreate() an existing Spark session is retrieved, if none exists, one is created\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[4]') \\\n",
    "                    .appName('DBiBD') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows/samples\n",
    "n=1000\n",
    "\n",
    "# number of colums/features\n",
    "k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.83281826, -6.73397765,  1.35729456])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random coefficients (betas)\n",
    "betas=(np.random.rand(k)-0.5)*20\n",
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.19605922,  0.47050515, -7.83438589])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random covariances (cov)\n",
    "cov=(np.random.rand(k)-0.5)*20\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an RDD with n vectors, each containing k entries, where each entry is generated from a standard-normal distribution\n",
    "# With spark.sparkContext the interface to the Spark cluster is realized. This allows to execute all necessary operations on the cluster\n",
    "data = RandomRDDs.normalVectorRDD(spark.sparkContext, n, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying this function yields an rdd where the first element is a vector with a moderate covariance structure and some added noise \n",
    "# for a more realistic setting, while the second element is the target variable.\n",
    "def createRow(noise):\n",
    "    x=[]\n",
    "    x.append(noise[0])\n",
    "    for i in range(1,len(noise)):\n",
    "        x.append((x[0]*cov[i])+noise[i])\n",
    "    x= [float(a) for a in x]\n",
    "    \n",
    "    y=0\n",
    "    for i in range(0,len(x)):\n",
    "        y+=x[i]*betas[i]\n",
    "    \n",
    "    return x,float(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of the createRow function to each element of the RDD (data)\n",
    "data=data.map(createRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.0606416 ,  0.48895633, -8.37592827],\n",
       "       [ 0.48895633,  1.20720599, -3.85092226],\n",
       "       [-8.37592827, -3.85092226, 67.19110856]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RowMatrix from the vectors of the RDD (data).\n",
    "# The RowMatrix offers the advantage that it is optimized for the application of operations from the linear algebra. With it an efficient\n",
    "# implementation of the calculations can be ensured without the need for additional effort to bring the data into the required format.\n",
    "dataMatrix=RowMatrix(data.map(lambda x : x[0]))\n",
    "\n",
    "# Calculation of covariance matrix and conversion to array\n",
    "dataMatrix.computeCovariance().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------+\n",
      "|features                              |y          |\n",
      "+--------------------------------------+-----------+\n",
      "|[0.099376045, -0.7133142, -0.9548688] |4.186422   |\n",
      "|[0.76255125, 0.19460212, -6.969095]   |-5.5591874 |\n",
      "|[-1.2307073, -1.5058644, 9.823388]    |15.064489  |\n",
      "|[-0.87878966, 0.29600775, 6.2963905]  |0.5481366  |\n",
      "|[0.57823384, 1.1535741, -3.0455763]   |-7.95092   |\n",
      "|[-0.18514186, -0.23748638, 1.4703904] |2.32994    |\n",
      "|[-0.10045725, 0.5425021, 1.9396323]   |-1.706951  |\n",
      "|[0.0780959, 0.2928381, -0.23282509]   |-1.7543622 |\n",
      "|[-2.037761, 0.7556917, 16.964283]     |4.0130663  |\n",
      "|[2.2538111, 1.0648041, -18.144684]    |-16.398167 |\n",
      "|[0.59066653, -0.056806386, -2.4121084]|1.1445082  |\n",
      "|[0.55334085, 0.98757243, -3.123839]   |-7.1093826 |\n",
      "|[-0.09503667, 0.8405007, 0.5635708]   |-5.5443497 |\n",
      "|[0.8281525, 1.0089866, -6.2490616]    |-9.617695  |\n",
      "|[-0.21122994, 1.2439487, 2.9526653]   |-5.8123817 |\n",
      "|[0.53106874, 1.8474259, -2.303149]    |-11.9378805|\n",
      "|[-0.42615938, -0.13071044, 2.5087743] |1.3734773  |\n",
      "|[-1.6975114, -2.0508745, 12.253665]   |18.84359   |\n",
      "|[-0.8513174, 0.09852043, 7.1540704]   |3.229849   |\n",
      "|[-0.48141384, -0.666, 5.5830894]      |8.773313   |\n",
      "+--------------------------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A schema is used to define the structure of a data frame.\n",
    "# The data frame contains the columns features and y.\n",
    "# The features column consists of an array of floats per row. It shows the values of the independent variables\n",
    "# The y column has a single float value per row and describes the dependent variable.\n",
    "schema = StructType([       \n",
    "    StructField('features', ArrayType(FloatType()), True),\n",
    "    StructField('y', FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create the data frame df using the schema defined at the beginning\n",
    "dataDF=data.toDF(schema=schema)\n",
    "# Display of the data frame\n",
    "dataDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark OLS: 9.319233655929565 seconds\n",
      "real values:\t\t [ 6.832818 -6.733978  1.357295]\n",
      "predicted values:\t [ 6.832818 -6.733978  1.357295]\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a LinearRegression instance to perform the linear regression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"y\", predictionCol=\"pred_y\")\n",
    "\n",
    "# Calculate the linear model. To use the method correctly, column features is converted to a vector format\n",
    "lr_model = lr.fit(dataDF.withColumn(\"features\",array_to_vector('features')))\n",
    "\n",
    "# Finally, the results of the OLS estimation, the true betas and the total execution time are output\n",
    "print(\"PySpark OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"predicted values:\\t\",lr_model.coefficients.round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.mllib.linalg.distributed.RowMatrix object at 0x0000016CCB980C40> 1000 x 3\n",
      "DenseMatrix([[  32.55140289,   15.000607  , -257.06243852],\n",
      "             [   0.        ,   31.35924019,    0.37105258],\n",
      "             [   0.        ,    0.        ,   32.33428266]])\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute the QR decomposition on the dataMatrix using the tallSkinnyQR method by calculating both matrices Q and R (True).\n",
    "QR=dataMatrix.tallSkinnyQR(True)\n",
    "\n",
    "# Output of the Q-matrix, the number of rows and columns \n",
    "print(QR.Q, QR.Q.numRows(),\"x\",QR.Q.numCols())\n",
    "\n",
    "# Print the R matrix.\n",
    "print(QR.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.03072064, -0.01469514,  0.24440239],\n",
       "        [ 0.        ,  0.03188853, -0.00036594],\n",
       "        [ 0.        ,  0.        ,  0.03092693]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion of the R matrix into a Numpy matrix\n",
    "R=np.asmatrix(QR.R.toArray())\n",
    "\n",
    "# Calculation of the inverse of R\n",
    "R_inv=np.linalg.inv(R)\n",
    "\n",
    "# Output of the inverse\n",
    "R_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CoordinateMatrix (cm) with the Q matrix (QR.Q) via \"MatrixEntry\" objects\n",
    "cm = CoordinateMatrix(\n",
    "    QR.Q.rows.zipWithIndex().flatMap(\n",
    "        lambda x: [MatrixEntry(x[1], j, v) for j, v in enumerate(x[0])]\n",
    "    )\n",
    ")\n",
    "# Build the transposed Q-matrix (Q_T) using the CoordinateMatrix (cm) and convert it to a RowMatrix (toRowMatrix())\n",
    "Q_T=cm.transpose().toRowMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DenseMatrix (y) is built with the parameters n, 1 and the values from the one-dimensional vector.\n",
    "# The one-dimensional vector is created from the \"y\" column of the data frame dataDF by converting it to a Numpy array.\n",
    "\n",
    "y=DenseMatrix(n,1,dataDF.select(\"y\").toPandas().to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QR OLS: 34.09499263763428 seconds\n",
      "real values:\t\t [ 6.832818 -6.733978  1.357295]\n",
      "QR predicted values:\t [ 6.832818 -6.733978  1.357295]\n"
     ]
    }
   ],
   "source": [
    "# Performing the matrix multiplications\n",
    "\n",
    "# Q_T and y are multiplied with the multiply function\n",
    "# With rows.collect() the calculations of the Spark driver nodes are returned to a local data structure, in this case a Python list\n",
    "step1 = Q_T.multiply(y).rows.collect() \n",
    "\n",
    "# Convert step1 to a numpy array\n",
    "step1 = np.array(step1)  \n",
    "\n",
    "# Calculate the betas\n",
    "step2 = np.matmul(R_inv, step1) \n",
    "\n",
    "# Finally, the results of the OLS estimation, the true betas and the total execution time are output\n",
    "print(\"QR OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\", betas.round(6))\n",
    "print(\"QR predicted values:\\t\", step2.ravel().round(6)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[261.52173163318855,31.38418119595542,4.0214241975753655]\n",
      "DenseMatrix([[-0.1235301 , -0.00194794,  0.99233891],\n",
      "             [-0.05758562, -0.99829884, -0.00912812],\n",
      "             [ 0.99066857, -0.05827205,  0.12320778]])\n",
      "<pyspark.mllib.linalg.distributed.RowMatrix object at 0x0000016CCDEA9A60> 1000 x 3\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute the singular value decomposition (SVD) of the data matrix dataMatrix with k singular values.\n",
    "# With computeU = True the both matrices U and V are calculated.\n",
    "svd=dataMatrix.computeSVD(k,computeU=True)\n",
    "\n",
    "# Output of the singular values\n",
    "print(svd.s)\n",
    "\n",
    "# Output of the matrix V\n",
    "print(svd.V)\n",
    "\n",
    "# Output of the matrix U and the properties of it\n",
    "print(svd.U, svd.U.numRows(),\"x\",svd.U.numCols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CoordinateMatrix (cm) with the U-matrix (svd.U) via \"MatrixEntry\" objects\n",
    "cm = CoordinateMatrix(\n",
    "    svd.U.rows.zipWithIndex().flatMap(\n",
    "        lambda x: [MatrixEntry(x[1], j, v) for j, v in enumerate(x[0])]\n",
    "    )\n",
    ")\n",
    "# Build the transposed U-matrix (U_T) using the CoordinateMatrix (cm) and convert it to a RowMatrix (toRowMatrix())\n",
    "U_T=cm.transpose().toRowMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD OLS: 31.016135692596436 seconds\n",
      "real values:\t\t [ 6.832818 -6.733978  1.357295]\n",
      "SVD predicted values:\t [ 6.832818 -6.733978  1.357295]\n"
     ]
    }
   ],
   "source": [
    "# U_T and y are multiplied with the multiply function\n",
    "# With rows.collect() the calculations of the Spark driver nodes are returned to a local data structure, in this case a Python list\n",
    "step1=U_T.multiply(y).rows.collect()\n",
    "\n",
    "# Calculate the OLS estimator according to calculation rule\n",
    "step2=(np.array(step1).ravel()/svd.s)\n",
    "v=np.matrix(svd.V.toArray())\n",
    "SVD_coeeffs=(v @ step2).ravel()\n",
    "\n",
    "# Finally, the results of the OLS estimation, the true betas and the total execution time are output\n",
    "print(\"SVD OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"SVD predicted values:\\t\",SVD_coeeffs.round(6)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU\n",
    "als Data Paralleism Ansatz da keine LU Funktion in PySpark enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luSpark(part):\n",
    "    # Die Größe der Partition ermitteln\n",
    "    partition_n = len(part)\n",
    "    \n",
    "    # Array Features in nparray schreiben damit die struktur mit .T transponiert werden kann\n",
    "    lufeatures = np.array([b for b in part[\"features\"].to_numpy()])\n",
    "    ypanda = part[\"y\"].to_numpy()\n",
    "\n",
    "    # Durchführen der LU-Composition\n",
    "    LUtemp, pivtemp = lu_factor(lufeatures.T @ lufeatures)\n",
    "    lubetas = lu_solve((LUtemp, pivtemp), lufeatures.T @ ypanda)\n",
    "    \n",
    "    # Eine DataFrame mit den geschätzten Betas und der Anzahl der Beobachtungen erstellen\n",
    "    return pd.DataFrame({\"betas\": [lubetas], \"sampleCounts\": [partition_n]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scheme\n",
    "schemaUDF=StructType([\n",
    "    StructField(\"betas\",ArrayType(FloatType())),\n",
    "    StructField(\"sampleCounts\",IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------+\n",
      "|betas                             |sampleCounts|\n",
      "+----------------------------------+------------+\n",
      "|[6.8325405, -6.7339754, 1.35726]  |250         |\n",
      "|[6.8327565, -6.7339783, 1.3572867]|250         |\n",
      "|[6.832314, -6.73398, 1.3572307]   |250         |\n",
      "|[6.833318, -6.7339807, 1.357357]  |250         |\n",
      "+----------------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the \"luSpark\" function to the partition groups of the \"dataDF\" DataFrame\n",
    "# Calculate and average the coefficients for each partition\n",
    "LU_res = dataDF.groupBy(F.spark_partition_id()).applyInPandas(luSpark,schema=schemaUDF)\n",
    "LU_res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weighted average of estimated betas for all partitions\n",
    "LU_betas = pd.DataFrame(LU_res.rdd.map(lambda x : [(x[\"sampleCounts\"]/n) * xi for xi in x[\"betas\"]]).collect()).sum().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LU OLS: 12.475306749343872 seconds\n",
      "real values:\t\t [ 6.832818 -6.733978  1.357295]\n",
      "LU predicted values :\t [ 6.832732 -6.733979  1.357284]\n"
     ]
    }
   ],
   "source": [
    "# Finally, the results of the OLS estimation, the true betas and the total execution time are output\n",
    "print(\"LU OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"LU predicted values :\\t\",LU_betas.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
