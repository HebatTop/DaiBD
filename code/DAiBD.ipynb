{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the environment variable for PySpark to the current path of the Python interpreter (sys.executable)\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries and modules\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, IndexedRowMatrix\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.mllib.linalg import DenseMatrix\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.linalg import lu, lu_factor, lu_solve # is used for LU decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and configuration of the Spark session.\n",
    "\n",
    "# Set the master node for Spark (running locally with 4 cores on a Spark standalone cluster).\n",
    "# Using .appName() to set the name for the application, which will be displayed on the user interface.\n",
    "# With .getOrCreate() an existing Spark session is retrieved, if none exists, one is created\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[4]') \\\n",
    "                    .appName('DBiBD') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of rows/samples\n",
    "n=1000\n",
    "\n",
    "# number of colums/features\n",
    "k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.97255868, -4.48318715,  0.17640443])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random coefficients (betas)\n",
    "betas=(np.random.rand(k)-0.5)*20\n",
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.41538752,  7.57379063, -1.66947667])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate random covariances (cov)\n",
    "cov=(np.random.rand(k)-0.5)*20\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an RDD with n vectors, each containing k entries, where each entry is generated from a standard-normal distribution\n",
    "# With spark.sparkContext the interface to the Spark cluster is realized. This allows to execute all necessary operations on the cluster\n",
    "data = RandomRDDs.normalVectorRDD(spark.sparkContext, n, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying this function yields an rdd where the first element is a vector with a moderate covariance structure and some added noise \n",
    "# for a more realistic setting, while the second element is the target variable.\n",
    "def createRow(noise):\n",
    "    x=[]\n",
    "    x.append(noise[0])\n",
    "    for i in range(1,len(noise)):\n",
    "        x.append((x[0]*cov[i])+noise[i])\n",
    "    x= [float(a) for a in x]\n",
    "    \n",
    "    y=0\n",
    "    for i in range(0,len(x)):\n",
    "        y+=x[i]*betas[i]\n",
    "    \n",
    "    return x,float(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of the createRow function to each element of the RDD (data)\n",
    "data=data.map(createRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.97927137,   7.44326431,  -1.62914619],\n",
       "       [  7.44326431,  57.58214759, -12.40458928],\n",
       "       [ -1.62914619, -12.40458928,   3.68981865]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RowMatrix from the vectors of the RDD (data).\n",
    "# The RowMatrix offers the advantage that it is optimized for the application of operations from the linear algebra. With it an efficient\n",
    "# implementation of the calculations can be ensured without the need for additional effort to bring the data into the required format.\n",
    "dataMatrix=RowMatrix(data.map(lambda x : x[0]))\n",
    "\n",
    "# Calculation of covariance matrix and conversion to array\n",
    "dataMatrix.computeCovariance().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------+\n",
      "|features                              |y         |\n",
      "+--------------------------------------+----------+\n",
      "|[1.2034985, 8.455216, -2.79036]       |-39.569023|\n",
      "|[0.9264144, 8.393093, -1.9257588]     |-38.86851 |\n",
      "|[1.3961579, 10.412863, -4.0280867]    |-48.751232|\n",
      "|[-1.0093409, -8.578743, -0.31200716]  |39.386715 |\n",
      "|[0.91097784, 9.545054, -0.86367697]   |-43.8306  |\n",
      "|[0.14880404, 1.4307998, -0.19924782]  |-6.5944123|\n",
      "|[0.030753141, 0.07893534, -1.1931145] |-0.5942618|\n",
      "|[-1.4463581, -10.559787, 2.5892231]   |49.204918 |\n",
      "|[0.034043737, -0.39865318, 0.08931492]|1.7698828 |\n",
      "|[-1.0986181, -9.4023695, 2.0095234]   |43.575542 |\n",
      "|[-0.9096731, -6.70736, 2.3814182]     |31.375153 |\n",
      "|[0.7321585, 4.9095526, -1.5275366]    |-22.991976|\n",
      "|[0.109175295, 0.95224166, 0.84459144] |-4.2262673|\n",
      "|[-0.1518071, -2.6367342, 1.4115095]   |12.217611 |\n",
      "|[0.28778645, 0.6972642, -0.7015823]   |-3.5296173|\n",
      "|[0.9538665, 6.452496, -2.0569303]     |-30.21829 |\n",
      "|[0.7995248, 5.0532165, -0.9965174]    |-23.607891|\n",
      "|[-0.4642001, -4.680366, 0.83354884]   |21.581459 |\n",
      "|[1.6959697, 13.453351, -1.8289812]    |-62.28596 |\n",
      "|[1.3152788, 12.438506, -2.9583216]    |-57.5652  |\n",
      "+--------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A schema is used to define the structure of a data frame.\n",
    "# The data frame contains the columns features and y.\n",
    "# The features column consists of an array of floats per row. It shows the values of the independent variables\n",
    "# The y column has a single float value per row and describes the dependent variable.\n",
    "schema = StructType([       \n",
    "    StructField('features', ArrayType(FloatType()), True),\n",
    "    StructField('y', FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create the data frame df using the schema defined at the beginning\n",
    "dataDF=data.toDF(schema=schema)\n",
    "# Display of the data frame\n",
    "dataDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark OLS: 8.726112604141235 seconds\n",
      "real values:\t\t [ 6.677466 -6.681536  6.642338]\n",
      "predicted values:\t [ 6.677466 -6.681536  6.642338]\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a LinearRegression instance to perform the linear regression\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"y\", predictionCol=\"pred_y\")\n",
    "\n",
    "# Calculate the linear model. To use the method correctly, column features is converted to a vector format\n",
    "lr_model = lr.fit(dataDF.withColumn(\"features\",array_to_vector('features')))\n",
    "\n",
    "# Finally, the results of the OLS estimation, the true betas and the total execution time are output\n",
    "print(\"PySpark OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"predicted values:\\t\",lr_model.coefficients.round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.mllib.linalg.distributed.RowMatrix object at 0x0000018F3194ECD0> 1000 x 3\n",
      "DenseMatrix([[-32.91823191, 170.68436231, 296.17565899],\n",
      "             [  0.        ,  30.89025415,  -0.96603129],\n",
      "             [  0.        ,   0.        , -31.06256769]])\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute the QR decomposition on the dataMatrix using the tallSkinnyQR method by calculating both matrices Q and R (True).\n",
    "QR=dataMatrix.tallSkinnyQR(True)\n",
    "\n",
    "# Output of the Q-matrix, the number of rows and columns \n",
    "print(QR.Q, QR.Q.numRows(),\"x\",QR.Q.numCols())\n",
    "\n",
    "# Print the R matrix.\n",
    "print(QR.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.0303783 ,  0.16785557, -0.29487155],\n",
       "        [ 0.        ,  0.03237267, -0.00100677],\n",
       "        [-0.        , -0.        , -0.03219309]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversion of the R matrix into a Numpy matrix\n",
    "R=np.asmatrix(QR.R.toArray())\n",
    "\n",
    "# Calculation of the inverse of R\n",
    "R_inv=np.linalg.inv(R)\n",
    "\n",
    "# Output of the inverse\n",
    "R_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CoordinateMatrix (cm) with the Q matrix (QR.Q) via \"MatrixEntry\" objects\n",
    "cm = CoordinateMatrix(\n",
    "    QR.Q.rows.zipWithIndex().flatMap(\n",
    "        lambda x: [MatrixEntry(x[1], j, v) for j, v in enumerate(x[0])]\n",
    "    )\n",
    ")\n",
    "# Build the transposed Q-matrix (Q_T) using the CoordinateMatrix (cm) and convert it to a RowMatrix (toRowMatrix())\n",
    "Q_T=cm.transpose().toRowMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The DenseMatrix (y) is built with the parameters n, 1 and the values from the one-dimensional vector.\n",
    "# The one-dimensional vector is created from the \"y\" column of the data frame dataDF by converting it to a Numpy array.\n",
    "\n",
    "y=DenseMatrix(n,1,dataDF.select(\"y\").toPandas().to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QR OLS: 28.485340356826782 seconds\n",
      "real values:\t\t [ 6.677466 -6.681536  6.642338]\n",
      "QR predicted values:\t [ 6.677466 -6.681537  6.642338]\n"
     ]
    }
   ],
   "source": [
    "# Performing the matrix multiplications\n",
    "\n",
    "# Q_T and y are multiplied with the multiply function\n",
    "# With rows.collect() the calculations of the Spark driver nodes are returned to a local data structure, in this case a Python list\n",
    "step1 = Q_T.multiply(y).rows.collect() \n",
    "\n",
    "# Convert step1 to a numpy array\n",
    "step1 = np.array(step1)  \n",
    "\n",
    "# Calculate the betas\n",
    "step2 = np.matmul(R_inv, step1) \n",
    "\n",
    "# Finally, the results of the OLS estimation, the true betas and the total execution time are output\n",
    "print(\"QR OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\", betas.round(6))\n",
    "print(\"QR predicted values:\\t\", step2.ravel().round(6)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[344.76853943586775,31.351581604986123,2.9221886574357034]\n",
      "DenseMatrix([[-0.09510553, -0.00195771, -0.99546527],\n",
      "             [ 0.49690419,  0.86641087, -0.04917753],\n",
      "             [ 0.86257821, -0.49932792, -0.08142766]])\n",
      "<pyspark.mllib.linalg.distributed.RowMatrix object at 0x0000018F33AD3D90> 1000 x 3\n"
     ]
    }
   ],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute the singular value decomposition (SVD) of the data matrix dataMatrix with k singular values.\n",
    "# With computeU = True the both matrices U and V are calculated.\n",
    "svd=dataMatrix.computeSVD(k,computeU=True)\n",
    "\n",
    "# Output of the singular values\n",
    "print(svd.s)\n",
    "\n",
    "# Output of the matrix V\n",
    "print(svd.V)\n",
    "\n",
    "# Output of the matrix U and the properties if it´s\n",
    "print(svd.U, svd.U.numRows(),\"x\",svd.U.numCols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a CoordinateMatrix (cm) with the U-matrix (svd.U) via \"MatrixEntry\" objects\n",
    "cm = CoordinateMatrix(\n",
    "    svd.U.rows.zipWithIndex().flatMap(\n",
    "        lambda x: [MatrixEntry(x[1], j, v) for j, v in enumerate(x[0])]\n",
    "    )\n",
    ")\n",
    "# Build the transposed U-matrix (U_T) using the CoordinateMatrix (cm) and convert it to a RowMatrix (toRowMatrix())\n",
    "U_T=cm.transpose().toRowMatrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD OLS: 24.881237030029297 seconds\n",
      "real values:\t\t [ 6.677466 -6.681536  6.642338]\n",
      "SVD predicted values:\t [ 6.677466 -6.681537  6.642338]\n"
     ]
    }
   ],
   "source": [
    "# U_T and y are multiplied with the multiply function\n",
    "# With rows.collect() the calculations of the Spark driver nodes are returned to a local data structure, in this case a Python list\n",
    "step1=U_T.multiply(y).rows.collect()\n",
    "\n",
    "# Calculate the OLS estimator according to calculation rule\n",
    "step2=(np.array(step1).ravel()/svd.s)\n",
    "v=np.matrix(svd.V.toArray())\n",
    "SVD_coeeffs=(v @ step2).ravel()\n",
    "\n",
    "# Finally, the results of the OLS estimation, the true betas and the total execution time are output\n",
    "print(\"SVD OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"SVD predicted values:\\t\",SVD_coeeffs.round(6)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU\n",
    "als Data Paralleism Ansatz da keine LU Funktion in PySpark enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luSpark(part):\n",
    "    # Die Größe der Partition ermitteln\n",
    "    partition_n = len(part)\n",
    "    \n",
    "    # Array Features in nparray schreiben damit die struktur mit .T transponiert werden kann\n",
    "    lufeatures = np.array([b for b in part[\"features\"].to_numpy()])\n",
    "    ypanda = part[\"y\"].to_numpy()\n",
    "\n",
    "    # Durchführen der LU-Composition\n",
    "    LUtemp, pivtemp = lu_factor(lufeatures.T @ lufeatures)\n",
    "    lubetas = lu_solve((LUtemp, pivtemp), lufeatures.T @ ypanda)\n",
    "    \n",
    "    # Eine DataFrame mit den geschätzten Betas und der Anzahl der Beobachtungen erstellen\n",
    "    return pd.DataFrame({\"betas\": [lubetas], \"sampleCounts\": [partition_n]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the scheme\n",
    "schemaUDF=StructType([\n",
    "    StructField(\"betas\",ArrayType(FloatType())),\n",
    "    StructField(\"sampleCounts\",IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\Casimir\\AppData\\Local\\Temp\\ipykernel_13532\\3263207875.py\", line 11, in luSpark\n  File \"c:\\Users\\Casimir\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\linalg\\_decomp_lu.py\", line 140, in lu_solve\n    raise ValueError(\"Shapes of lu {} and b {} are incompatible\"\nValueError: Shapes of lu (3, 3) and b (1, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Casimir\\IONOS HiDrive\\Uni\\FH SWF\\fhswfds\\DatenanalyseInBigData\\1a\\DAiBD.ipynb Cell 36\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Casimir/IONOS%20HiDrive/Uni/FH%20SWF/fhswfds/DatenanalyseInBigData/1a/DAiBD.ipynb#Y134sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m LU_res \u001b[39m=\u001b[39m dataDF\u001b[39m.\u001b[39mgroupBy(F\u001b[39m.\u001b[39mspark_partition_id())\u001b[39m.\u001b[39mapplyInPandas(luSpark,schema\u001b[39m=\u001b[39mschemaUDF)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Casimir/IONOS%20HiDrive/Uni/FH%20SWF/fhswfds/DatenanalyseInBigData/1a/DAiBD.ipynb#Y134sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m LU_res\u001b[39m.\u001b[39;49mshow(truncate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\dataframe.py:912\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    905\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    906\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m         },\n\u001b[0;32m    910\u001b[0m     )\n\u001b[1;32m--> 912\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, int_truncate, vertical))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\Casimir\\AppData\\Local\\Temp\\ipykernel_13532\\3263207875.py\", line 11, in luSpark\n  File \"c:\\Users\\Casimir\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\linalg\\_decomp_lu.py\", line 140, in lu_solve\n    raise ValueError(\"Shapes of lu {} and b {} are incompatible\"\nValueError: Shapes of lu (3, 3) and b (1, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Apply the \"luSpark\" function to the partition groups of the \"dataDF\" DataFrame\n",
    "# Calculate and average the coefficients for each partition\n",
    "LU_res = dataDF.groupBy(F.spark_partition_id()).applyInPandas(luSpark,schema=schemaUDF)\n",
    "LU_res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate weighted average of estimated betas for all partitions\n",
    "LU_betas = pd.DataFrame(LU_res.rdd.map(lambda x : [(x[\"sampleCounts\"]/n) * xi for xi in x[\"betas\"]]).collect()).sum().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LU OLS: 32.74242973327637 seconds\n",
      "real values:\t\t [ 6.677466 -6.681536  6.642338]\n",
      "LU predicted values :\t [ 6.677938 -6.681465  6.642348]\n"
     ]
    }
   ],
   "source": [
    "# Finally, the results of the OLS estimation, the true betas and the total execution time are output\n",
    "print(\"LU OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"LU predicted values :\\t\",LU_betas.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End Spark Session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
