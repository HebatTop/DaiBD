{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, IndexedRowMatrix\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.mllib.linalg import DenseMatrix\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.linalg import lu, lu_factor, lu_solve # wird f√ºr LU-Decomposition genutzt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs=1\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[4]') \\\n",
    "                    .appName('DBiBD') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000\n",
    "k=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.97255868, -4.48318715,  0.17640443])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas=(np.random.rand(k)-0.5)*20\n",
    "betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.41538752,  7.57379063, -1.66947667])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov=(np.random.rand(k)-0.5)*20\n",
    "cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = RandomRDDs.normalVectorRDD(spark.sparkContext, n, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRow(noise):\n",
    "    x=[]\n",
    "    x.append(noise[0])\n",
    "    for i in range(1,len(noise)):\n",
    "        x.append((x[0]*cov[i])+noise[i])\n",
    "    x= [float(a) for a in x]\n",
    "    \n",
    "    y=0\n",
    "    for i in range(0,len(x)):\n",
    "        y+=x[i]*betas[i]\n",
    "    \n",
    "    return x,float(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.map(createRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.97927137,   7.44326431,  -1.62914619],\n",
       "       [  7.44326431,  57.58214759, -12.40458928],\n",
       "       [ -1.62914619, -12.40458928,   3.68981865]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataMatrix=RowMatrix(data.map(lambda x : x[0]))\n",
    "dataMatrix.computeCovariance().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------+\n",
      "|features                              |y         |\n",
      "+--------------------------------------+----------+\n",
      "|[1.2034985, 8.455216, -2.79036]       |-39.569023|\n",
      "|[0.9264144, 8.393093, -1.9257588]     |-38.86851 |\n",
      "|[1.3961579, 10.412863, -4.0280867]    |-48.751232|\n",
      "|[-1.0093409, -8.578743, -0.31200716]  |39.386715 |\n",
      "|[0.91097784, 9.545054, -0.86367697]   |-43.8306  |\n",
      "|[0.14880404, 1.4307998, -0.19924782]  |-6.5944123|\n",
      "|[0.030753141, 0.07893534, -1.1931145] |-0.5942618|\n",
      "|[-1.4463581, -10.559787, 2.5892231]   |49.204918 |\n",
      "|[0.034043737, -0.39865318, 0.08931492]|1.7698828 |\n",
      "|[-1.0986181, -9.4023695, 2.0095234]   |43.575542 |\n",
      "|[-0.9096731, -6.70736, 2.3814182]     |31.375153 |\n",
      "|[0.7321585, 4.9095526, -1.5275366]    |-22.991976|\n",
      "|[0.109175295, 0.95224166, 0.84459144] |-4.2262673|\n",
      "|[-0.1518071, -2.6367342, 1.4115095]   |12.217611 |\n",
      "|[0.28778645, 0.6972642, -0.7015823]   |-3.5296173|\n",
      "|[0.9538665, 6.452496, -2.0569303]     |-30.21829 |\n",
      "|[0.7995248, 5.0532165, -0.9965174]    |-23.607891|\n",
      "|[-0.4642001, -4.680366, 0.83354884]   |21.581459 |\n",
      "|[1.6959697, 13.453351, -1.8289812]    |-62.28596 |\n",
      "|[1.3152788, 12.438506, -2.9583216]    |-57.5652  |\n",
      "+--------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([       \n",
    "    StructField('features', ArrayType(FloatType()), True),\n",
    "    StructField('y', FloatType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "dataDF=data.toDF(schema=schema)\n",
    "dataDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark OLS: 8.726112604141235 seconds\n",
      "real values:\t\t [ 6.677466 -6.681536  6.642338]\n",
      "predicted values:\t [ 6.677466 -6.681536  6.642338]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"y\", predictionCol=\"pred_y\")\n",
    "lr_model = lr.fit(dataDF.withColumn(\"features\",array_to_vector('features')))\n",
    "print(\"PySpark OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"predicted values:\\t\",lr_model.coefficients.round(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.mllib.linalg.distributed.RowMatrix object at 0x0000018F3194ECD0> 1000 x 3\n",
      "DenseMatrix([[-32.91823191, 170.68436231, 296.17565899],\n",
      "             [  0.        ,  30.89025415,  -0.96603129],\n",
      "             [  0.        ,   0.        , -31.06256769]])\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "QR=dataMatrix.tallSkinnyQR(True)\n",
    "print(QR.Q, QR.Q.numRows(),\"x\",QR.Q.numCols())\n",
    "print(QR.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[-0.0303783 ,  0.16785557, -0.29487155],\n",
       "        [ 0.        ,  0.03237267, -0.00100677],\n",
       "        [-0.        , -0.        , -0.03219309]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R=np.asmatrix(QR.R.toArray())\n",
    "R_inv=np.linalg.inv(R)\n",
    "R_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoordinateMatrix(\n",
    "    QR.Q.rows.zipWithIndex().flatMap(\n",
    "        lambda x: [MatrixEntry(x[1], j, v) for j, v in enumerate(x[0])]\n",
    "    )\n",
    ")\n",
    "Q_T=cm.transpose().toRowMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=DenseMatrix(n,1,dataDF.select(\"y\").toPandas().to_numpy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# betas = R_inv Q_T y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QR OLS: 28.485340356826782 seconds\n",
      "real values:\t\t [ 6.677466 -6.681536  6.642338]\n",
      "QR predicted values:\t [ 6.677466 -6.681537  6.642338]\n"
     ]
    }
   ],
   "source": [
    "step1=Q_T.multiply(y).rows.collect()\n",
    "step1=np.array(step1)\n",
    "step2= np.matmul(R_inv,step1)\n",
    "\n",
    "print(\"QR OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"QR predicted values:\\t\",step2.ravel().round(6)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[344.76853943586775,31.351581604986123,2.9221886574357034]\n",
      "DenseMatrix([[-0.09510553, -0.00195771, -0.99546527],\n",
      "             [ 0.49690419,  0.86641087, -0.04917753],\n",
      "             [ 0.86257821, -0.49932792, -0.08142766]])\n",
      "<pyspark.mllib.linalg.distributed.RowMatrix object at 0x0000018F33AD3D90> 1000 x 3\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "svd=dataMatrix.computeSVD(k,computeU=True)\n",
    "print(svd.s)\n",
    "print(svd.V)\n",
    "print(svd.U, svd.U.numRows(),\"x\",svd.U.numCols())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = CoordinateMatrix(\n",
    "    svd.U.rows.zipWithIndex().flatMap(\n",
    "        lambda x: [MatrixEntry(x[1], j, v) for j, v in enumerate(x[0])]\n",
    "    )\n",
    ")\n",
    "U_T=cm.transpose().toRowMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#betas = V * ((U^T * y) / s element wise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD OLS: 24.881237030029297 seconds\n",
      "real values:\t\t [ 6.677466 -6.681536  6.642338]\n",
      "SVD predicted values:\t [ 6.677466 -6.681537  6.642338]\n"
     ]
    }
   ],
   "source": [
    "step1=U_T.multiply(y).rows.collect()\n",
    "step2=(np.array(step1).ravel()/svd.s)\n",
    "v=np.matrix(svd.V.toArray())\n",
    "\n",
    "SVD_coeeffs=(v @ step2).ravel()\n",
    "print(\"SVD OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"SVD predicted values:\\t\",SVD_coeeffs.round(6)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LU\n",
    "als Data Paralleism Ansatz da keine LU Funktion in PySpark enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luSpark(part):\n",
    "    # Die Gr√∂√üe der Partition ermitteln\n",
    "    partition_n = len(part)\n",
    "    \n",
    "    # Array Features in nparray schreiben damit die struktur mit .T transponiert werden kann\n",
    "    lufeatures = np.array([b for b in part[\"features\"].to_numpy()])\n",
    "    ypanda = part[\"y\"].to_numpy()\n",
    "\n",
    "    # Durchf√ºhren der LU-Composition\n",
    "    LUtemp, pivtemp = lu_factor(lufeatures.T @ lufeatures)\n",
    "    lubetas = lu_solve((LUtemp, pivtemp), lufeatures.T @ ypanda)\n",
    "    \n",
    "    # Eine DataFrame mit den gesch√§tzten Betas und der Anzahl der Beobachtungen erstellen\n",
    "    return pd.DataFrame({\"betas\": [lubetas], \"sampleCounts\": [partition_n]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaUDF=StructType([\n",
    "    StructField(\"betas\",ArrayType(FloatType())),\n",
    "    StructField(\"sampleCounts\",IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f√ºr jede Partition die Koeffizienten berechnen und mitteln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\Casimir\\AppData\\Local\\Temp\\ipykernel_13532\\3263207875.py\", line 11, in luSpark\n  File \"c:\\Users\\Casimir\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\linalg\\_decomp_lu.py\", line 140, in lu_solve\n    raise ValueError(\"Shapes of lu {} and b {} are incompatible\"\nValueError: Shapes of lu (3, 3) and b (1, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Casimir\\IONOS HiDrive\\Uni\\FH SWF\\fhswfds\\DatenanalyseInBigData\\1a\\DAiBD.ipynb Cell 36\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Casimir/IONOS%20HiDrive/Uni/FH%20SWF/fhswfds/DatenanalyseInBigData/1a/DAiBD.ipynb#Y134sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m LU_res \u001b[39m=\u001b[39m dataDF\u001b[39m.\u001b[39mgroupBy(F\u001b[39m.\u001b[39mspark_partition_id())\u001b[39m.\u001b[39mapplyInPandas(luSpark,schema\u001b[39m=\u001b[39mschemaUDF)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Casimir/IONOS%20HiDrive/Uni/FH%20SWF/fhswfds/DatenanalyseInBigData/1a/DAiBD.ipynb#Y134sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m LU_res\u001b[39m.\u001b[39;49mshow(truncate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\sql\\dataframe.py:912\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m    904\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    905\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    906\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    909\u001b[0m         },\n\u001b[0;32m    910\u001b[0m     )\n\u001b[1;32m--> 912\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, int_truncate, vertical))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\Casimir\\AppData\\Local\\Temp\\ipykernel_13532\\3263207875.py\", line 11, in luSpark\n  File \"c:\\Users\\Casimir\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\linalg\\_decomp_lu.py\", line 140, in lu_solve\n    raise ValueError(\"Shapes of lu {} and b {} are incompatible\"\nValueError: Shapes of lu (3, 3) and b (1, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LU_res = dataDF.groupBy(F.spark_partition_id()).applyInPandas(luSpark,schema=schemaUDF)\n",
    "\n",
    "LU_res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted average berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LU_betas = pd.DataFrame(LU_res.rdd.map(lambda x : [(x[\"sampleCounts\"]/n) * xi for xi in x[\"betas\"]]).collect()).sum().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LU OLS: 32.74242973327637 seconds\n",
      "real values:\t\t [ 6.677466 -6.681536  6.642338]\n",
      "LU predicted values :\t [ 6.677938 -6.681465  6.642348]\n"
     ]
    }
   ],
   "source": [
    "print(\"LU OLS: %s seconds\" % (time.time() - start_time))\n",
    "print(\"real values:\\t\\t\",betas.round(6))\n",
    "print(\"LU predicted values :\\t\",LU_betas.round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
