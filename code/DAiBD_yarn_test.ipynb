{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "# Import the required libraries and modules\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.mllib.random import RandomRDDs\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, IntegerType\n",
    "from pyspark.mllib.linalg.distributed import RowMatrix, IndexedRowMatrix\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.functions import array_to_vector\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix, MatrixEntry\n",
    "from pyspark.mllib.linalg import DenseMatrix\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy.linalg import lu, lu_factor, lu_solve # is used for LU decomposition\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "\n",
    "The following script varies the cluster nodes, the n rows and the k columns, and the regression estimator to compare the different regression estimators. The outermost loop iterates over the cluster nodes and configures the Spark session. The inner loops iterate over the different combinations of n and k. Within the inner loop, the dataset is additionally simulated and applied based on the flags (do_Spark, do_QR ...) from the different regression estimators.\n",
    "Finally, still within the innermost loop, the measured execution times as well as the parameters are stored in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_jobs = 64 n = 6000 k = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LU MAE:  1.4370528813641842e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_jobs = 64 n = 6000 k = 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LU MAE:  0.00011964710042418058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_jobs = 64 n = 6000 k = 10\n",
      "LU MAE:  1.820609616253188e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_jobs = 64 n = 6000 k = 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LU MAE:  0.00012059048529337556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Festlegen des Regressionsschätzers\n",
    "\n",
    "do_Spark=False\n",
    "do_QR=False\n",
    "do_SVD=False\n",
    "do_LU=True\n",
    "\n",
    "# Festlegen der parallelen Clusterknoten\n",
    "n_jobs_a = [64]#, 32, 8, 1]\n",
    "\n",
    "# Anzahl der Wiederholungen für jede OLS-Schätzung bei den gleichen Parametern für n und k\n",
    "numtries = 10\n",
    "\n",
    "for n_jobs in n_jobs_a:\n",
    "    # Initialization and configuration of the Spark session.\n",
    "    # Spark is run in yarn-cluster distributed cluster mode.\n",
    "    # In summary, the clusters are configured, resource and memory management are set\n",
    "    spark = SparkSession.builder \\\n",
    "                        .master('yarn') \\\n",
    "                        .appName('DBiBD') \\\n",
    "                        .config(\"spark.driver.port\", \"55500\")\\\n",
    "                        .config(\"spark.driver.memory\", \"15g\")\\\n",
    "                        .config(\"spark.driver.blockManager.port\", \"55501\")\\\n",
    "                        .config(\"spark.executor.instances\", n_jobs)\\\n",
    "                        .config(\"spark.executor.cores\", 1)\\\n",
    "                        .config(\"spark.executor.memory\", \"15g\")\\\n",
    "                        .config(\"spark.executor.memoryOverhead\", \"15g\")\\\n",
    "                        .getOrCreate()\n",
    "\n",
    "    # Parameters for the possible dimensions of the data matrix\n",
    "    perms = [\n",
    "        # {\"n\": 1e2, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 3e2, \"k\": [2, 5, 10, 20]},\n",
    "        {\"n\": 6e3, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 1e4, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 3e4, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 6e4, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 1e5, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 3e5, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 6e5, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 1e6, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 6e6, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 1e7, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 6e7, \"k\": [2, 5, 10, 20]},\n",
    "        # {\"n\": 1e9, \"k\": [2, 5, 10, 20]},\n",
    "    ]\n",
    "    perms.reverse()\n",
    "\n",
    "    # Loop over the parameter list perms\n",
    "    for nk in perms:\n",
    "        # number of rows/samples\n",
    "        n = int(nk[\"n\"])\n",
    "        for k in nk[\"k\"]:\n",
    "            # number of colums/features\n",
    "            k = int(k)\n",
    "            # # Generate random coefficients (betas) and covariances (cov)\n",
    "            betas = (np.random.rand(k)-0.5)*20\n",
    "            cov = (np.random.rand(k)-0.5)*20\n",
    "            betas = (np.random.rand(k)-0.5)*20\n",
    "            cov = (np.random.rand(k)-0.5)*20\n",
    "\n",
    "            # Generate dataset\n",
    "            data = RandomRDDs.normalVectorRDD(spark.sparkContext, n, k)\n",
    "\n",
    "            def createRow(noise):\n",
    "                x = []\n",
    "                x.append(noise[0])\n",
    "                for i in range(1, len(noise)):\n",
    "                    x.append((x[0]*cov[i])+noise[i])\n",
    "                x = [float(a) for a in x]\n",
    "\n",
    "                y = 0\n",
    "                for i in range(0, len(x)):\n",
    "                    y += x[i]*betas[i]\n",
    "\n",
    "                return x, float(y)\n",
    "            data = data.map(createRow)\n",
    "            dataMatrix = RowMatrix(data.map(lambda x: x[0]))\n",
    "            schema = StructType([\n",
    "                StructField('features', ArrayType(FloatType()), True),\n",
    "                StructField('y', FloatType(), True)\n",
    "            ])\n",
    "\n",
    "            dataDF = data.toDF(schema=schema)\n",
    "            print(\"n_jobs =\", n_jobs, \"n =\", n, \"k =\", k)\n",
    "            \n",
    "            # Iteration over the required repetitions.\n",
    "            # repetitions for the same calculation serves to ensure certain variance and to take into account statistical uncertainty\n",
    "            for numtry in range(numtries):\n",
    "                \n",
    "                # PySpark Linear Regression\n",
    "                if do_Spark:\n",
    "                    start_time = time.time()\n",
    "                    lr = LinearRegression(featuresCol=\"features\", labelCol=\"y\", predictionCol=\"pred_y\",regParam=0.001)\n",
    "                    lr_model = lr.fit(dataDF.withColumn(\"features\", array_to_vector('features')))\n",
    "                    time_PySpark = time.time() - start_time\n",
    "                    \n",
    "                # QR\n",
    "                if do_QR:\n",
    "                    start_time = time.time()\n",
    "                    QR = dataMatrix.tallSkinnyQR(True)\n",
    "                    R = np.asmatrix(QR.R.toArray())\n",
    "                    R_inv = np.linalg.inv(R)\n",
    "                    cm = CoordinateMatrix(\n",
    "                        QR.Q.rows.zipWithIndex().flatMap(\n",
    "                            lambda x: [MatrixEntry(x[1], j, v) for j, v in enumerate(x[0])]\n",
    "                        )\n",
    "                    )\n",
    "                    Q_T = cm.transpose().toRowMatrix()\n",
    "                    y = DenseMatrix(n, 1, dataDF.select(\"y\").toPandas().to_numpy().ravel())\n",
    "                    step1 = Q_T.multiply(y).rows.collect()\n",
    "                    step1 = np.array(step1)\n",
    "                    step2 = np.matmul(R_inv, step1)\n",
    "                    time_QR = time.time() - start_time\n",
    "                # SVD\n",
    "                if do_SVD:\n",
    "                    start_time = time.time()\n",
    "                    svd = dataMatrix.computeSVD(k, computeU=True)\n",
    "                    cm = CoordinateMatrix(\n",
    "                        svd.U.rows.zipWithIndex().flatMap(\n",
    "                            lambda x: [MatrixEntry(x[1], j, v) for j, v in enumerate(x[0])]\n",
    "                        )\n",
    "                    )\n",
    "                    U_T = cm.transpose().toRowMatrix()\n",
    "\n",
    "                    step1 = U_T.multiply(y).rows.collect()\n",
    "                    step2 = (np.array(step1).ravel()/svd.s)\n",
    "                    v = np.matrix(svd.V.toArray())\n",
    "\n",
    "                    SVD_coeeffs = (v @ step2).ravel()\n",
    "                    time_SVD = time.time() - start_time\n",
    "                \n",
    "                #LU\n",
    "                if do_LU:\n",
    "                    start_time = time.time()\n",
    "                    def luSpark(part):\n",
    "                        # Die Größe der Partition ermitteln\n",
    "                        partition_n = len(part)\n",
    "                        \n",
    "                        # Array Features in nparray schreiben damit die struktur mit .T transponiert werden kann\n",
    "                        lufeatures = np.array([b for b in part[\"features\"].to_numpy()])\n",
    "                        ypanda = part[\"y\"].to_numpy()\n",
    "\n",
    "                        # Durchführen der LU-Composition\n",
    "                        LUtemp, pivtemp = lu_factor(lufeatures.T @ lufeatures)\n",
    "                        lubetas = lu_solve((LUtemp, pivtemp), lufeatures.T @ ypanda)\n",
    "                        \n",
    "                        # Eine DataFrame mit den geschätzten Betas und der Anzahl der Beobachtungen erstellen\n",
    "                        return pd.DataFrame({\"betas\": [lubetas], \"sampleCounts\": [partition_n]})\n",
    "                    \n",
    "                    schemaUDF=StructType([\n",
    "                        StructField(\"betas\",ArrayType(FloatType())),\n",
    "                        StructField(\"sampleCounts\",IntegerType())\n",
    "                    ])\n",
    "                    LU_res = dataDF.groupBy(F.spark_partition_id()).applyInPandas(luSpark,schema=schemaUDF)\n",
    "                    try:\n",
    "                        LU_betas = pd.DataFrame(LU_res.rdd.map(lambda x : [(x[\"sampleCounts\"]/n) * xi for xi in x[\"betas\"]]).collect()).sum().to_numpy()\n",
    "                    except Exception as e:\n",
    "                        print (e)\n",
    "                    \n",
    "                    time_LU=time.time() - start_time\n",
    "                    if numtry == 0:\n",
    "                        print(\"LU MAE: \",mean_absolute_error(betas,LU_betas))\n",
    "\n",
    "                # Saving the results to results.csv \n",
    "                with open('cgi/DAiBD/results.csv', 'a') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    if do_Spark:\n",
    "                        writer.writerow([n_jobs, n, k, numtry, \"PySpark\", time_PySpark])\n",
    "                    if do_QR:\n",
    "                        writer.writerow([n_jobs, n, k, numtry, \"QR\", time_QR])\n",
    "                    if do_SVD:\n",
    "                        writer.writerow([n_jobs, n, k, numtry, \"SVD\", time_SVD])\n",
    "                    if do_LU:\n",
    "                        writer.writerow([n_jobs, n, k, numtry, \"LU\", time_LU])\n",
    "    \n",
    "    # Ending the Spark session\n",
    "    spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
